tags:: draft, [[bias]], [[conjecture]]

- People overfit to spurious associations, leading to bad decisions when the association does not happen.
  collapsed:: true
	- {{tweet https://twitter.com/trylks/status/1363851701028081665}}
- Some examples:
	- Short and [[simple]]: The amount of information in a short explanation will be small, making it seemingly [[simple]]. However, there are several reasons this may work the other way: #.v-numlist
		- Producing something [[simple]] is in fact hard, ((64da6481-1c86-4df2-bfb7-3ff53145b080)).
		- Short is more likely to be ambiguous. Something ambiguous may be understood in several ways.
		  * ((62cc4b57-bb38-420e-a95c-c3a10dcf3731))
		  * ((80661c3c-61f4-4272-a992-a4015270e1da))
			- Finding at least one of them is more likely than for something unambiguous, creating the [[illusion]] of understanding and [[simplicity]].
			- Finding the right one is less likely than for something unambiguous, hence why the understanding is a [[illusion]].
		- Kolmogorov [[complexity]] is undecidable, and it is hard to estimate it, so we use concision as a [[proxy]], but it is a spurious association.
		- If a long explanation includes information that is not understood, or considerations that fly above someoneâ€™s head, they will seem irrelevant, pointless and unnecessary, i.e. they will look like [[unnecessary complexity]].
		  * ((63adbb54-979c-404c-ae71-6b75c859bdf2))
	- [[Simple]] and understanding: True understanding results in [[informed simplicity]], but [[naive simplicity]] results from a lack of understanding. [[Complex]], in a [[midwit]] fashion, implies greater understanding than [[naive simplicity]].
	- Understanding and true:  #.v-numlist
		- *[[Ad verecundiam]]* [[fallacy]].
		- e.g. good understanding of something fictional, e.g. any disproved scientific theory.
		- Confounding factor: not true $\rightarrow$ contradictory $\rightarrow$ confusing $\rightarrow$ no understanding
	- True and useful: correct information may result in negative utility, especially when misused.
		- Usual example: disregarding tail risks with low probability, but catastrophic consequences if materialised, especially when preparing for those risks is a small investment. Taleb has written in detail about non-ergodic processes too.
-
-